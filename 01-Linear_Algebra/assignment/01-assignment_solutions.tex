\documentclass[12pt]{article}
\usepackage{../../includes/lecture_notes}
\usepackage{../../includes/math}
\usepackage{../../includes/uark_colors}

\hypersetup{
  colorlinks = true,
  allcolors = ozark_mountains,
  breaklinks = true,
  bookmarksopen = true
}
\newcommand{\answer}[1]{{\color{blue_winged_teal}\textbf{Answer:} #1}}

\begin{document}
\begin{center}
  {\Huge\bf Topic \#1 Assignment}
  
  \smallskip
  {\large\it  ECON 5753 â€” University of Arkansas}

  \medskip
  {\large Prof. Kyle Butts}
\end{center}

These assignments should be completed in groups of 1 or 2 but submitted individually. 

\section*{Theoretical Questions}

\begin{enumerate}
  \item Let 
  $$
    \bm{A} = \begin{bmatrix} 6 & 7 & 9 \\ 1 & 2 & 3 \\ 8 & 4 & 6 \end{bmatrix}, 
    \quad 
    \bm{B} = \begin{bmatrix} 10 & 9 & 8 \\ 7 & 5 & 4 \\ 1 & 7 & 6 \end{bmatrix},
    \quad \text{ and } 
    \bm{C} = \begin{bmatrix} 1 & 0 & 2 \\ 0 & 1 & 0 \\ 4 & 0 & 2 \end{bmatrix}.
  $$ 
  Calculate the following:
  
  \begin{enumerate}
    \item $3\bm{B}^T + \bm{A}$
    
    \answer{
      $$
        \begin{bmatrix}
          36 & 28 & 12 \\ 
          28 & 17 & 24 \\ 
          32 & 16 & 24 
        \end{bmatrix}
      $$
    }
    
    \item $\bm{C}^T - 4\bm{A}$
    
    \answer{
      $$
        \begin{bmatrix}
          -23 & -28 & -32 \\ 
          -4 & -7 & -12 \\ 
          -30 & -16 & -22
        \end{bmatrix}
      $$
    }
    
    \item $(\bm{C}\bm{A})'$
    
    \answer{
      $$
        \begin{bmatrix}
          22 & 1 & 40\\
          15 & 2 & 36\\
          21 & 3 & 48
        \end{bmatrix}
      $$
    }
  \end{enumerate}


  \bigskip\bigskip
  \item Let 
  $$
    \bm{X} = \begin{bmatrix} x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33} \end{bmatrix}, 
    \quad 
    \beta = \begin{bmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{bmatrix}.
  $$ 
  Verify the following two are equivalent:

  $$
    X \beta \quad \text{ and } 
    X_{\cdot, 1} \beta_1 + X_{\cdot, 2} \beta_2 + X_{\cdot, 3} \beta_3 
  $$

  \answer{
    \begin{align*}
      X \beta = 
      \begin{bmatrix}
        x_{11} & x_{12} & x_{13} \\ x_{21} & x_{22} & x_{23} \\ x_{31} & x_{32} & x_{33}
      \end{bmatrix} 
      \begin{bmatrix} \beta_1 \\ \beta_2 \\ \beta_3 \end{bmatrix}
      = 
      \begin{bmatrix}
        x_{11} \beta_1 + x_{12} \beta_2 + x_{13} \beta_3 \\ 
        x_{21} \beta_1 + x_{22} \beta_2 + x_{23} \beta_3 \\
        x_{31} \beta_1 + x_{32} \beta_2 + x_{33} \beta_3
      \end{bmatrix} 
    \end{align*}

    \begin{align*}
      X_{\cdot, 1} \beta_1 + X_{\cdot, 2} \beta_2 + X_{\cdot, 3} \beta_3 &=
      \begin{bmatrix}
        x_{11} \\ x_{21} \\ x_{31} 
      \end{bmatrix} \beta_1 +
      \begin{bmatrix}
        x_{12} \\ x_{22} \\ x_{32} 
      \end{bmatrix} \beta_2 +
      \begin{bmatrix}
        x_{13} \\ x_{23} \\ x_{33} 
      \end{bmatrix} \beta_3
      =
      \begin{bmatrix}
        x_{11} \beta_1 + x_{12} \beta_2 + x_{13} \beta_3 \\ 
        x_{21} \beta_1 + x_{22} \beta_2 + x_{23} \beta_3 \\
        x_{31} \beta_1 + x_{32} \beta_2 + x_{33} \beta_3
      \end{bmatrix} 
    \end{align*}
  }

  
  \newpage
  \item Match each matrix with its inverse:
  
  $$
    A = \begin{bmatrix} 1 & 2 & -1 \\ -2 & 0 & 1 \\ 1 & -1 & 0 \end{bmatrix}, 
    \quad 
    B = \begin{bmatrix} 2 & 4 & 1 \\ -1 & 1 & -1 \\ 1 & 4 & 0 \end{bmatrix}, 
    \quad \text{ and } 
    C = \begin{bmatrix}  1 & 2 & 3 \\ 2 & 4 & 0 \\ 0 & 0 & 3 \end{bmatrix}
  $$
  
  $$
    a = \begin{bmatrix} -4 & -4 & 5 \\ 1 & 1 & -1 \\ 5 & 4 & -6 \end{bmatrix}, 
    \quad 
    b = \begin{bmatrix} 1 & 1 & 2 \\ 1 & 1 & 1 \\ 2 & 3 & 4\end{bmatrix}, 
    \quad \text{ and } 
    c = \text{ Not invertible}.
  $$

  \answer{
    A and b; B and a; C and c. You can verify these by doing e.g. $Ab = bA = I$.
  }


  \bigskip\bigskip
  \item Let $$
    \varepsilon = \begin{bmatrix}\varepsilon_1 \\ \varepsilon_2\end{bmatrix} \sim \mathcal{N}\left( \begin{bmatrix}1 \\ 1\end{bmatrix}, \begin{bmatrix}\sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2\end{bmatrix} \right).
  $$
  
  What is the distribution of $\begin{bmatrix} 1 & 1 \end{bmatrix} \varepsilon$?

  \answer{
    Note $\begin{bmatrix} 1 & 1 \end{bmatrix} \varepsilon = \varepsilon_1 + \varepsilon_2$ is the sum of the two errors. The expectation is $\expec{\varepsilon_1 + \varepsilon_2} = \expec{\varepsilon_1} + \expec{\varepsilon_2} = 2$.

    The variance is 
    $$
      \begin{bmatrix} 1 & 1 \end{bmatrix} 
      \begin{bmatrix}\sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2\end{bmatrix}
      \begin{bmatrix} 1 \\ 1 \end{bmatrix} = 
      \begin{bmatrix} 1 & 1 \end{bmatrix} 
      \begin{bmatrix}\sigma_1^2 + \sigma_{12} \\ \sigma_{12} + \sigma_2^2\end{bmatrix} = \sigma_1^2 + 2\sigma_{12} + \sigma_2^2
    $$

    Therefore, we have $\begin{bmatrix} 1 & 1 \end{bmatrix} \varepsilon \sim \mathcal{N}(2, \sigma_1^2 + 2\sigma_{12} + \sigma_2^2)$.
  }

  \bigskip\bigskip
  \item Let $f(x, y) = x^2 + y^2 + 2xy$.
  \begin{enumerate}
    \item What is $\frac{\partial}{\partial x} f(x,y)$ and $\frac{\partial}{\partial y} f(x,y)$?
    
    \answer{
      $\frac{\partial}{\partial x} f(x,y) = 2x + 2y$ and 
      $\frac{\partial}{\partial y} f(x,y) = 2y + 2x$
    }
    
    \item Use this to create a taylor approximation of this function at $(x_0, y_0)$. Write this in the form of:
    $$
      f(x_0 + dx, y_0 + dy) \approx f(x_0, y_0) + \begin{bmatrix}\frac{\partial}{\partial x} f(x,y) & \frac{\partial}{\partial x} f(x,y)\end{bmatrix} \begin{bmatrix}dx \\ dy\end{bmatrix}
    $$

    \answer{
      \begin{align*}
        f(x_0 + dx, y_0 + dy) &\approx f(x_0, y_0) + \begin{bmatrix}\frac{\partial}{\partial x} f(x,y) & \frac{\partial}{\partial x} f(x,y)\end{bmatrix} \begin{bmatrix}dx \\ dy\end{bmatrix} \\
        &= f(x_0, y_0) + \begin{bmatrix} 2x_0 + 2y_0 \\ 2y_0 + 2x_0 \end{bmatrix} \begin{bmatrix}dx \\ dy\end{bmatrix} \\
        &= x_0^2 + y_0^2 + 2x_0y_0 + (2x_0 + 2y_0) dx + (2y_0 + 2x_0) dy
      \end{align*}
    }

    \item Plug in $(x_0, y_0) = (0, 0)$ to create a linear approximation to this function.
    \answer{
      Evaluating the above expansion at $(x_0, y_0) = (0, 0)$ yields
      $$
        f(x_0 + dx, y_0 + dy) = 0
      $$
      Since $(0, 0)$ is the minimum of this function, the local linear approximation of it at this point is just the flat plane.
    }
  \end{enumerate}
\end{enumerate}


\end{document}
